{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e64fe208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Federated Learning Simulation...\n",
      "\n",
      "Round 1\n",
      "Completed round 1 of training\n",
      "\n",
      "Round 2\n",
      "Completed round 2 of training\n",
      "\n",
      "Round 3\n",
      "Completed round 3 of training\n",
      "\n",
      "Round 4\n",
      "Completed round 4 of training\n",
      "\n",
      "Round 5\n",
      "Completed round 5 of training\n",
      "\n",
      "Test prediction for 'Hello how are':\n",
      "Predicted next words: ['you']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, clone_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "\n",
    "class UserDevice:\n",
    "    def __init__(self, user_id: int, local_data: List[str], tokenizer: Tokenizer, \n",
    "                 vocab_size: int, max_sequence_length: int):\n",
    "        \"\"\"Initialize a user device with local data and model.\"\"\"\n",
    "        self.user_id = user_id\n",
    "        self.local_data = local_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.model = self._create_model()\n",
    "        \n",
    "    def _create_model(self) -> Sequential:\n",
    "        \"\"\"Create the local model architecture.\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.vocab_size, 100, input_length=self.max_sequence_length-1),\n",
    "            LSTM(100, return_sequences=True),\n",
    "            LSTM(100),\n",
    "            Dense(100, activation='relu'),\n",
    "            Dense(self.vocab_size, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def prepare_sequences(self) -> tuple:\n",
    "        \"\"\"Prepare input sequences and target words from local data.\"\"\"\n",
    "        sequences = self.tokenizer.texts_to_sequences(self.local_data)\n",
    "        \n",
    "        input_sequences = []\n",
    "        for sequence in sequences:\n",
    "            for i in range(1, len(sequence)):\n",
    "                n_gram_sequence = sequence[:i+1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "        \n",
    "        padded_sequences = pad_sequences(\n",
    "            input_sequences, \n",
    "            maxlen=self.max_sequence_length, \n",
    "            padding='pre'\n",
    "        )\n",
    "        \n",
    "        X = padded_sequences[:, :-1]\n",
    "        y = padded_sequences[:, -1]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train_local_model(self, epochs=1, batch_size=32):\n",
    "        \"\"\"Train the local model on user's data.\"\"\"\n",
    "        X, y = self.prepare_sequences()\n",
    "        if len(X) > 0:\n",
    "            self.model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    def get_model_weights(self) -> List:\n",
    "        \"\"\"Return the local model weights.\"\"\"\n",
    "        return self.model.get_weights()\n",
    "    \n",
    "    def set_model_weights(self, weights: List):\n",
    "        \"\"\"Update local model with global weights.\"\"\"\n",
    "        self.model.set_weights(weights)\n",
    "\n",
    "class FederatedServer:\n",
    "    def __init__(self, model_template: Sequential):\n",
    "        \"\"\"Initialize the federated learning server.\"\"\"\n",
    "        self.global_model = model_template\n",
    "        \n",
    "    def aggregate_weights(self, local_weights: List[List]) -> List:\n",
    "        \"\"\"Aggregate local model weights using FedAvg algorithm.\"\"\"\n",
    "        # Simple averaging of weights\n",
    "        averaged_weights = []\n",
    "        for weights_list_tuple in zip(*local_weights):\n",
    "            averaged_weights.append(\n",
    "                np.array([np.array(w).mean(axis=0) for w in zip(*weights_list_tuple)])\n",
    "            )\n",
    "        return averaged_weights\n",
    "\n",
    "def load_data_from_file(file_path: str) -> List[List[str]]:\n",
    "    \"\"\"Load user data from a text file where each line is a sentence.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Assuming each line is a sentence and splitting them into chunks for different users.\n",
    "    # For simplicity here we assign every 5 lines to one user.\n",
    "    user_data = [lines[i:i + 5] for i in range(0, len(lines), 5)]\n",
    "    \n",
    "    # Strip any extra whitespace or newline characters.\n",
    "    user_data = [[line.strip() for line in user_lines] for user_lines in user_data]\n",
    "    \n",
    "    return user_data\n",
    "\n",
    "def simulate_federated_learning():\n",
    "    # Load user data from text file (replace 'user_data.txt' with your actual file path)\n",
    "    user_data_file_path = 'C:\\\\Users\\\\Alireza217\\\\Desktop\\\\user_data.txt.txt'\n",
    "    user_data = load_data_from_file(user_data_file_path)\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    all_texts = [text for user_texts in user_data for text in user_texts]\n",
    "    tokenizer.fit_on_texts(all_texts)\n",
    "    \n",
    "    # Parameters\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    max_sequence_length = max(len(text.split()) for text in all_texts) + 1\n",
    "    \n",
    "    # Create template model for server\n",
    "    template_model = Sequential([\n",
    "        Embedding(vocab_size, 100, input_length=max_sequence_length-1),\n",
    "        LSTM(100, return_sequences=True),\n",
    "        LSTM(100),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    template_model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Initialize devices and server\n",
    "    devices = [\n",
    "        UserDevice(i, data, tokenizer, vocab_size, max_sequence_length)\n",
    "        for i, data in enumerate(user_data)\n",
    "    ]\n",
    "    server = FederatedServer(template_model)\n",
    "    \n",
    "    # Training parameters\n",
    "    n_rounds = 5\n",
    "    local_epochs = 3\n",
    "    \n",
    "    print(\"Starting Federated Learning Simulation...\")\n",
    "    \n",
    "    for round_num in range(n_rounds):\n",
    "        print(f\"\\nRound {round_num + 1}\")\n",
    "        \n",
    "        # Local training on each device\n",
    "        local_weights = []\n",
    "        for device in devices:\n",
    "            # Train local model\n",
    "            device.train_local_model(epochs=local_epochs)\n",
    "            \n",
    "            # Send model weights to server\n",
    "            local_weights.append(device.get_model_weights())\n",
    "        \n",
    "        # Server aggregates weights\n",
    "        global_weights = server.aggregate_weights(local_weights)\n",
    "        \n",
    "        # Devices update their models with global weights\n",
    "        for device in devices:\n",
    "            device.set_model_weights(global_weights)\n",
    "        \n",
    "        print(f\"Completed round {round_num + 1} of training\")\n",
    "\n",
    "    def predict_next_word(device, text, num_predictions=1):\n",
    "        \"\"\"Predict the next word given a text input.\"\"\"\n",
    "        sequence = device.tokenizer.texts_to_sequences([text])[0]\n",
    "        padded = pad_sequences([sequence], maxlen=device.max_sequence_length-1, padding='pre')\n",
    "        predictions = device.model.predict(padded, verbose=0)[0]\n",
    "        \n",
    "        top_indices = predictions.argsort()[-num_predictions:][::-1]\n",
    "        \n",
    "        words = []\n",
    "        for idx in top_indices:\n",
    "            for word, index in device.tokenizer.word_index.items():\n",
    "                if index == idx:\n",
    "                    words.append(word)\n",
    "                    break\n",
    "        return words\n",
    "\n",
    "    # Test the model with a sample prediction\n",
    "    test_device = devices[0]\n",
    "    test_text = \"Hello how are\"\n",
    "    predictions = predict_next_word(test_device, test_text)\n",
    "    print(f\"\\nTest prediction for '{test_text}':\")\n",
    "    print(f\"Predicted next words: {predictions}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    simulate_federated_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2735068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
